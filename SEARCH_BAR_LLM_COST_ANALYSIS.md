# üîç Search Bar LLM Cost Analysis

## **Current Implementation**

### **LLM Usage: YES**

**Service**: OpenRouter with `gpt-4o-mini`
**Location**: `services/content-service/src/services/workflowIntentService.js`
**Model**: `openai/gpt-4o-mini`
**Max Tokens**: 1000
**Temperature**: 0.3

**Code:**
```javascript
const response = await openrouterClient.chat.completions.create({
  model: 'openai/gpt-4o-mini',
  messages: [
    { role: 'system', content: 'You are a professional intent analysis assistant. Respond only with valid JSON.' },
    { role: 'user', content: prompt }
  ],
  temperature: 0.3,
  max_tokens: 1000
});
```

---

## **When Analysis is Triggered**

### **Current Behavior: On Every Input**

**Trigger**: `@input="handleInput"` (every keystroke)
**Condition**: `if (searchQuery.value.length >= 2)`

**Flow:**
1. User types "a" ‚Üí No analysis (length < 2)
2. User types "am" ‚Üí **LLM call triggered** ‚úÖ
3. User types "ama" ‚Üí **LLM call triggered** ‚úÖ
4. User types "amaz" ‚Üí **LLM call triggered** ‚úÖ
5. User types "amazo" ‚Üí **LLM call triggered** ‚úÖ
6. User types "amazon" ‚Üí **LLM call triggered** ‚úÖ

**Problem**: **5 LLM calls** for typing "amazon" (6 characters)

---

## **Cost Analysis**

### **OpenRouter gpt-4o-mini Pricing (Approximate)**

**Input**: ~$0.15 per 1M tokens
**Output**: ~$0.60 per 1M tokens

### **Estimated Cost Per Call**

**Prompt Size**: ~500-800 tokens (system + user prompt)
**Response Size**: ~200-400 tokens (JSON response)
**Total**: ~700-1200 tokens per call

**Cost Per Call**: ~$0.0005 - $0.001 (very small, but adds up)

### **Real-World Cost Example**

**Scenario**: User types "amazon swe interview questions"
- Characters: 30
- LLM calls: ~28 calls (one per character after 2nd)
- **Cost**: ~$0.014 - $0.028 per search

**If 100 users search per day:**
- Searches: 100
- Average queries: 20 characters
- LLM calls: ~1,800 calls/day
- **Daily cost**: ~$0.90 - $1.80
- **Monthly cost**: ~$27 - $54

**If 1,000 users search per day:**
- **Monthly cost**: ~$270 - $540

---

## **How Other Companies Handle This**

### **1. GitHub Search:**
- **Approach**: Debounced search (500ms delay)
- **Implementation**: Only searches after user stops typing for 500ms
- **LLM Usage**: No LLM for search, uses keyword matching
- **Cost**: $0 (no LLM)

### **2. Notion Search:**
- **Approach**: Debounced search (300ms delay)
- **Implementation**: Waits 300ms after last keystroke
- **LLM Usage**: No LLM for search, uses full-text search
- **Cost**: $0 (no LLM)

### **3. Linear Search:**
- **Approach**: Debounced search (400ms delay)
- **Implementation**: Waits 400ms after last keystroke
- **LLM Usage**: No LLM for search, uses semantic search (embeddings)
- **Cost**: $0 (no LLM, uses embeddings)

### **4. VS Code Search:**
- **Approach**: Debounced search (500ms delay)
- **Implementation**: Waits 500ms after last keystroke
- **LLM Usage**: No LLM for search
- **Cost**: $0 (no LLM)

### **5. Google Search:**
- **Approach**: Debounced autocomplete (300ms delay)
- **Implementation**: Waits 300ms after last keystroke
- **LLM Usage**: No LLM for autocomplete (uses keyword matching)
- **Cost**: $0 (no LLM)

### **6. Perplexity / AI Search:**
- **Approach**: On-Enter only
- **Implementation**: Only searches when user presses Enter
- **LLM Usage**: Yes, but only on explicit search
- **Cost**: Controlled (only when user explicitly searches)

### **7. ChatGPT Search:**
- **Approach**: On-Enter only
- **Implementation**: Only processes when user presses Enter
- **LLM Usage**: Yes, but only on explicit action
- **Cost**: Controlled (only when user explicitly queries)

---

## **Best Practices**

### **Option 1: Debounce (Recommended)**
- **Approach**: Wait 500-800ms after user stops typing
- **Implementation**: Use `debounce` utility
- **Benefits**: 
  - Reduces LLM calls by ~80-90%
  - Still feels responsive
  - User sees results after they finish typing
- **Cost Reduction**: ~80-90% reduction

### **Option 2: On-Enter Only**
- **Approach**: Only analyze when user presses Enter
- **Implementation**: Move analysis to `@keydown.enter` handler
- **Benefits**:
  - Maximum cost reduction (~95%+)
  - User explicitly triggers search
  - No wasted calls
- **Cost Reduction**: ~95%+ reduction
- **Trade-off**: Less "magical" UX, requires explicit action

### **Option 3: Hybrid (Debounce + On-Enter)**
- **Approach**: Debounce for autocomplete, on-enter for full analysis
- **Implementation**: 
  - Debounced simple matching (no LLM) for suggestions
  - LLM analysis only on Enter
- **Benefits**:
  - Fast autocomplete (no LLM cost)
  - Full analysis on demand
- **Cost Reduction**: ~90%+ reduction

---

## **Recommended Solution**

### **Option A: Debounce (Best UX, Good Cost Reduction)**

**Implementation:**
```typescript
import { debounce } from 'lodash-es' // or use custom debounce

const debouncedAnalyzeIntent = debounce(async () => {
  await analyzeIntent()
}, 600) // 600ms delay

async function handleInput() {
  if (searchQuery.value.length >= 2) {
    showDropdown.value = true
    debouncedAnalyzeIntent() // Debounced call
  } else {
    showDropdown.value = false
    intentResult.value = null
  }
}
```

**Benefits:**
- ‚úÖ Reduces LLM calls by ~80-90%
- ‚úÖ Still feels responsive
- ‚úÖ User sees results after they finish typing
- ‚úÖ Better UX than on-enter only

**Cost Reduction**: ~80-90%

---

### **Option B: On-Enter Only (Maximum Cost Reduction)**

**Implementation:**
```typescript
// Remove @input="handleInput" from input
// Keep only @keydown.enter="handleEnter"

async function handleEnter() {
  if (searchQuery.value.length >= 2) {
    showDropdown.value = true
    await analyzeIntent() // Only on Enter
  }
}

// Show example prompts when input is focused but no query yet
function handleFocus() {
  if (!searchQuery.value && examplePrompts.value.length > 0) {
    showDropdown.value = true // Show examples
  }
}
```

**Benefits:**
- ‚úÖ Maximum cost reduction (~95%+)
- ‚úÖ User explicitly triggers search
- ‚úÖ No wasted calls
- ‚úÖ Clear user intent

**Cost Reduction**: ~95%+

**Trade-off**: Less "magical" UX, requires explicit action

---

## **Cost Comparison**

### **Current (On Every Input):**
- **Typing "amazon"**: 5 LLM calls
- **Cost per search**: ~$0.0025 - $0.005
- **100 searches/day**: ~$0.25 - $0.50/day
- **Monthly**: ~$7.50 - $15

### **With Debounce (600ms):**
- **Typing "amazon"**: 1 LLM call (after user stops typing)
- **Cost per search**: ~$0.0005 - $0.001
- **100 searches/day**: ~$0.05 - $0.10/day
- **Monthly**: ~$1.50 - $3

### **With On-Enter Only:**
- **Typing "amazon"**: 0 LLM calls (until Enter pressed)
- **Cost per search**: ~$0.0005 - $0.001 (only when Enter pressed)
- **100 searches/day**: ~$0.05 - $0.10/day
- **Monthly**: ~$1.50 - $3

**Savings**: 
- **Debounce**: ~80% cost reduction
- **On-Enter**: ~95%+ cost reduction

---

## **Recommendation**

### **Recommended: Debounce (600ms delay)**

**Why:**
1. **Best Balance**: Good UX + significant cost reduction
2. **Industry Standard**: Most apps use debounce (GitHub, Notion, Linear, VS Code)
3. **User Experience**: Still feels responsive, results appear after user finishes typing
4. **Cost Effective**: ~80-90% reduction in LLM calls

**Implementation Priority:**
1. **High**: Implement debounce immediately (easy fix, big savings)
2. **Medium**: Consider on-enter as alternative if cost is still too high
3. **Low**: Hybrid approach (more complex, but best of both worlds)

---

## **Summary**

**Current State:**
- ‚úÖ Using LLM: Yes (OpenRouter gpt-4o-mini)
- ‚ùå Trigger: On every input (expensive)
- ‚ùå No debouncing: Every keystroke triggers LLM call
- ‚ùå Cost: ~$0.0025 - $0.005 per search

**Recommended Fix:**
- ‚úÖ Debounce: 600ms delay after user stops typing
- ‚úÖ Cost Reduction: ~80-90%
- ‚úÖ Better UX: Results appear after user finishes typing
- ‚úÖ Industry Standard: Matches how GitHub, Notion, Linear handle this

**Alternative:**
- ‚úÖ On-Enter Only: Maximum cost reduction (~95%+)
- ‚úÖ Trade-off: Less "magical" UX, requires explicit action
