# üîç ML Services Analysis - How They Work & What's Needed

## **Current Setup (Local):**

### **1. Embedding-Server** üìä

**How it works:**
- Uses `sentence-transformers` library (Python)
- Model: `BAAI/bge-small-en-v1.5` (384 dimensions)
- **Downloads model automatically** from HuggingFace when you call `SentenceTransformer(MODEL_NAME)`
- Model is cached in `/root/.cache/huggingface` (Docker volume)
- **No API key needed** - downloads model files directly (not using API)

**Local setup:**
- Model downloads on first run (takes 5-10 minutes)
- Cached in Docker volume for faster subsequent runs

**Code:**
```python
# app.py line 29
model = SentenceTransformer(MODEL_NAME)  # Downloads from HuggingFace automatically
```

---

### **2. NER-Service** üè∑Ô∏è

**How it works:**
- Uses `transformers` library (Python)
- Model: `dslim/bert-base-NER`
- **Downloads model automatically** from HuggingFace when you call `pipeline("ner", model="...")`
- **No API key needed** - downloads model files directly

**Local setup:**
- You have a local model at: `/Users/luan02/Desktop/models/dslim-bert-base-NER`
- Docker volume mounts it: `/Users/luan02/Desktop/models/dslim-bert-base-NER:/app/models/bert-base-NER:ro`
- Code tries local first, falls back to HuggingFace download if not found

**Code:**
```python
# main.py line 26-34
if os.path.exists(model_path):
    ner_pipeline = pipeline("ner", model=model_path, ...)  # Use local
else:
    ner_pipeline = pipeline("ner", model="dslim/bert-base-NER", ...)  # Download from HuggingFace
```

---

### **3. Content-Service** üîó

**How it connects:**
- `EMBEDDING_PROVIDER = 'local'` - uses embedding-server (not HuggingFace API)
- Has `HUGGINGFACE_API_KEY` but **only for fallback** (if local server fails)
- Calls: `http://embedding-server:5000/embed` (local) or HuggingFace API (fallback)
- Calls: `http://ner-service:8000/extract-metadata` (local)

---

## **What Happens on Railway:**

### **Embedding-Server on Railway:**

‚úÖ **Will work automatically:**
1. Railway builds Docker image
2. Service starts
3. `SentenceTransformer('BAAI/bge-small-en-v1.5')` is called
4. **Model downloads from HuggingFace automatically** (first startup takes 5-10 minutes)
5. Model is cached in container (or you can use Railway volumes)
6. **No API key needed** - it's downloading model files, not using API

**What you need:**
- ‚úÖ Just deploy the service
- ‚úÖ Set `MODEL_NAME=BAAI/bge-small-en-v1.5` (already in env file)
- ‚ùå **No HuggingFace API key needed** (for embedding-server)

---

### **NER-Service on Railway:**

‚úÖ **Will work automatically:**
1. Railway builds Docker image
2. Service starts
3. Code checks for local model at `/app/models/bert-base-NER`
4. **Local model won't exist** (it's on your laptop)
5. Falls back to: `pipeline("ner", model="dslim/bert-base-NER")`
6. **Model downloads from HuggingFace automatically** (first startup takes 5-10 minutes)
7. **No API key needed** - it's downloading model files, not using API

**What you need:**
- ‚úÖ Just deploy the service
- ‚úÖ Set `MODEL_NAME=dslim/bert-base-NER` (already in env file)
- ‚ùå **No HuggingFace API key needed** (for ner-service)
- ‚úÖ **Dockerfile already downloads model during build** (line 15 in Dockerfile)

---

### **Content-Service on Railway:**

**What you need:**
- ‚úÖ Set `EMBEDDING_SERVER_URL=http://embedding-server` (internal service discovery)
- ‚úÖ Set `NER_SERVICE_URL=http://ner-service` (internal service discovery)
- ‚úÖ Keep `HUGGINGFACE_API_KEY` (for fallback if services fail)
- ‚úÖ Keep `EMBEDDING_PROVIDER=local` (uses embedding-server, not API)

---

## **Key Differences:**

### **Model Download vs API:**

| Method | How It Works | API Key Needed? | Cost |
|--------|-------------|-----------------|------|
| **Model Download** | Downloads model files from HuggingFace | ‚ùå NO | ‚úÖ Free |
| **HuggingFace API** | Calls HuggingFace Inference API | ‚úÖ YES | üí∞ Pay per request |

### **Your Setup:**

- **Embedding-Server:** Uses **model download** (free, no API key)
- **NER-Service:** Uses **model download** (free, no API key)
- **Content-Service Fallback:** Uses **HuggingFace API** (needs API key, costs money)

---

## **What Other Companies Do:**

### **Option 1: Download Models in Docker (Most Common)** ‚úÖ

**What they do:**
- Download models during Docker build
- Include models in Docker image
- **Pros:** Faster startup, no download on first request
- **Cons:** Larger Docker images

**Example:**
```dockerfile
# Download model during build
RUN python -c "from transformers import pipeline; pipeline('ner', model='dslim/bert-base-NER')"
```

**Your setup:** ‚úÖ Already doing this for NER (Dockerfile line 15)

---

### **Option 2: Download Models on First Request**

**What they do:**
- Download models when service starts
- Cache in container or volume
- **Pros:** Smaller Docker images
- **Cons:** Slower first request (5-10 minutes)

**Your setup:** ‚úÖ Embedding-server does this (downloads on startup)

---

### **Option 3: Use HuggingFace Inference API**

**What they do:**
- Call HuggingFace API instead of running models locally
- **Pros:** No infrastructure to manage
- **Cons:** Costs money, slower, rate limits

**Your setup:** ‚ö†Ô∏è Only as fallback (good!)

---

## **Summary - What You Need:**

### **For Embedding-Server:**
- ‚úÖ Deploy to Railway
- ‚úÖ Set `MODEL_NAME=BAAI/bge-small-en-v1.5`
- ‚ùå **No HuggingFace API key needed** (downloads model files, not using API)
- ‚è≥ First startup: Model downloads automatically (5-10 minutes)

### **For NER-Service:**
- ‚úÖ Deploy to Railway
- ‚úÖ Set `MODEL_NAME=dslim/bert-base-NER`
- ‚ùå **No HuggingFace API key needed** (downloads model files, not using API)
- ‚úÖ Model downloads during build (already in Dockerfile)
- ‚è≥ First startup: Model already downloaded (faster!)

### **For Content-Service:**
- ‚úÖ Set `EMBEDDING_SERVER_URL=http://embedding-server`
- ‚úÖ Set `NER_SERVICE_URL=http://ner-service`
- ‚úÖ Keep `HUGGINGFACE_API_KEY` (for fallback only)
- ‚úÖ Keep `EMBEDDING_PROVIDER=local`

---

## **About Your Local NER Model:**

**Current situation:**
- You have model at: `/Users/luan02/Desktop/models/dslim-bert-base-NER`
- Docker volume mounts it locally
- **This won't be available on Railway** (it's on your laptop)

**What happens on Railway:**
- Code checks for local model ‚Üí Not found
- Falls back to HuggingFace download ‚Üí ‚úÖ Works!
- Model downloads automatically ‚Üí ‚úÖ No problem!

**You don't need to:**
- ‚ùå Upload model to Railway
- ‚ùå Set up volumes
- ‚ùå Do anything special

**It just works!** The code already handles this fallback.

---

## **Quick Answer:**

**Q: Do I need HuggingFace API key for embedding-server/ner-service?**
**A: NO!** They download models directly (free), not using API.

**Q: What about my local NER model?**
**A: Not needed!** Code falls back to HuggingFace download automatically.

**Q: Will services work on Railway?**
**A: YES!** Models download automatically on first startup.

**Q: Do I need to do anything special?**
**A: NO!** Just deploy - models download automatically.

---

## **What Happens on First Deployment:**

### **Embedding-Server:**
1. Service starts
2. `SentenceTransformer('BAAI/bge-small-en-v1.5')` called
3. Model downloads from HuggingFace (5-10 minutes)
4. Model cached in container
5. Service ready! ‚úÖ

### **NER-Service:**
1. Docker build runs
2. `pipeline('ner', model='dslim/bert-base-NER')` called during build
3. Model downloads during build (10-15 minutes)
4. Model included in Docker image
5. Service starts quickly! ‚úÖ

**Both work automatically - no manual steps needed!** üéâ
